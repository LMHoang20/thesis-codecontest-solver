{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets peft accelerate bitsandbytes trl safetensors ipywidgets huggingface_hub scipy -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import psycopg2\n",
    "from huggingface_hub import login\n",
    "from constants import *\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "login('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')\n",
    "\n",
    "def load_model() -> tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "    model_name = \"google/gemma-2b\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from constants import *\n",
    "\n",
    "# The model that you want to train from the Hugging Face hub\n",
    "model_id = \"google/gemma-2b\"\n",
    "# The instruction dataset to use\n",
    "dataset_name = EDITORIAL_DATASET\n",
    "# Dataset split\n",
    "dataset_split = \"train\"\n",
    "# Fine-tuned model name\n",
    "new_model = \"gemma-2b-editorial\"\n",
    "# Huggingface repository\n",
    "hf_model_repo = GENERATOR_MODEL_ID\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = \"auto\"\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_double_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "# LoRA attention dimension\n",
    "lora_r = 16\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.05\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = new_model\n",
    "# Number of training epochs\n",
    "num_train_epochs = 2\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = True\n",
    "bf16 = False\n",
    "# Batch size per GPU for training\n",
    "batch_size = 128\n",
    "per_device_train_batch_size = 1\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = batch_size // per_device_train_batch_size\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = False\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 3e-4 #1e-5\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"constant\" # \"constant\"\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = False\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 0\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "# Disable tqdm\n",
    "disable_tqdm = True\n",
    "\n",
    "################################################################################\n",
    "# SFTTrainer parameters\n",
    "################################################################################\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = 512 #None\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = True #True #False\n",
    "\n",
    "# High-level parameters\n",
    "\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_use_double_quant=use_double_nested_quant,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=lora_dropout,\n",
    "        r=lora_r,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\n",
    "                \"q_proj\",\n",
    "                \"k_proj\",\n",
    "                \"v_proj\",\n",
    "                \"o_proj\",\n",
    "        ],\n",
    ")\n",
    "\n",
    "# Define the training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size, # 6 if use_flash_attention else 4,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    "    optim=optim,\n",
    "    #save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    #max_steps=max_steps,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    disable_tqdm=disable_tqdm,\n",
    "    report_to=\"tensorboard\",\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import gc\n",
    "\n",
    "from inference import load_model\n",
    "from settings import *\n",
    "from constants import *\n",
    "\n",
    "def format_instruction(sample):\n",
    "    name = sample['name']\n",
    "    description = sample['description']\n",
    "    tags = sample['tags']\n",
    "    editorial = sample['editorial']\n",
    "    sections = editorial.split('TUTORIAL CODE XXX')\n",
    "    solution = sections[0].strip()\n",
    "    code = sections[1].strip()\n",
    "    return f\"\"\"<start_of_turn>user\n",
    "You are a contestant in a programming contest. You are given the following complicated competitive programming problem:\n",
    "The problem description is given between the <DESCRIPTION> and </DESCRIPTION> tags.\n",
    "\n",
    "Here is your task:\n",
    "- Reason about the problem, write the editorial for the problem, wrapped in the <EDITORIAL> and </EDITORIAL> tags.\n",
    "- Write a solution in any programming language, preferably in Python/C++, wrapped in the <CODE> and </CODE> tags.\n",
    "\n",
    "Your entire answer should be wrapped in the <ANSWER> and </ANSWER> tags. Answer in Markdown format.\n",
    "\n",
    "<DESCRIPTION>\n",
    "Name: {name}\n",
    "Tags: {tags}\n",
    "{description}\n",
    "</DESCRIPTION><end_of_turn>\n",
    "<start_of_turn>model\n",
    "<ANSWER>\n",
    "<EDITORIAL>\n",
    "{solution}\n",
    "</EDITORIAL>\n",
    "<CODE>\n",
    "{code}\n",
    "</CODE>\n",
    "</ANSWER><end_of_turn>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model()\n",
    "\n",
    "DATASET_ID = EDITORIAL_DATASET\n",
    "SPLIT = 'train'\n",
    "CACHE_DIR = 'cache-editorial'\n",
    "\n",
    "dataset = datasets.load_dataset(DATASET_ID)\n",
    "\n",
    "dataset = dataset.filter(lambda x: x['has_code'])\n",
    "\n",
    "print(format_instruction(dataset['train'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=packing,\n",
    "    formatting_func=format_instruction,\n",
    "    args=args,\n",
    ")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "trainer.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfcrawler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
