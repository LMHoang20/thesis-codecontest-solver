{"cells":[{"cell_type":"markdown","metadata":{"id":"uTAiuz2uvPng"},"source":["# Instruction fine-tuning a Llama-2 model on generating Python code"]},{"cell_type":"markdown","metadata":{"id":"malpM0sXFn9K"},"source":["## Installing and loading the libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T16:30:29.955844Z","iopub.status.busy":"2023-12-20T16:30:29.955050Z","iopub.status.idle":"2023-12-20T16:31:07.448363Z","shell.execute_reply":"2023-12-20T16:31:07.447443Z","shell.execute_reply.started":"2023-12-20T16:30:29.955806Z"},"id":"bvHV20Q0vGkE","outputId":"ae8bb04d-0afe-4a74-d1b7-fe998ed5526a","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.35.0)\n","Collecting transformers\n","  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/20/0a/739426a81f7635b422fbe6cb8d1d99d1235579a6ac8024c13d743efa6847/transformers-4.36.2-py3-none-any.whl.metadata\n","  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\n","Collecting datasets\n","  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/e2/cf/db41e572d7ed958e8679018f8190438ef700aeb501b62da9e1eed9e4d69a/datasets-2.15.0-py3-none-any.whl.metadata\n","  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\n","Collecting peft\n","  Obtaining dependency information for peft from https://files.pythonhosted.org/packages/8b/1b/aee2a330d050c493642d59ba6af51f3910cb138ea48ede228c84c204a5af/peft-0.7.1-py3-none-any.whl.metadata\n","  Downloading peft-0.7.1-py3-none-any.whl.metadata (25 kB)\n","Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.24.1)\n","Collecting accelerate\n","  Obtaining dependency information for accelerate from https://files.pythonhosted.org/packages/f7/fc/c55e5a2da345c9a24aa2e1e0f60eb2ca290b6a41be82da03a6d4baec4f99/accelerate-0.25.0-py3-none-any.whl.metadata\n","  Downloading accelerate-0.25.0-py3-none-any.whl.metadata (18 kB)\n","Collecting bitsandbytes\n","  Obtaining dependency information for bitsandbytes from https://files.pythonhosted.org/packages/d9/8d/b62d4fb02587e293e5b91b68bbcaa2d88c6a0360b622e9521d4bd07a20cd/bitsandbytes-0.41.3.post2-py3-none-any.whl.metadata\n","  Downloading bitsandbytes-0.41.3.post2-py3-none-any.whl.metadata (9.8 kB)\n","Collecting trl\n","  Obtaining dependency information for trl from https://files.pythonhosted.org/packages/0d/44/c406c3cf5981bddb16ff72acb5ca235888db4073d868cf51bd143bef3aad/trl-0.7.4-py3-none-any.whl.metadata\n","  Downloading trl-0.7.4-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (0.4.0)\n","Collecting safetensors\n","  Obtaining dependency information for safetensors from https://files.pythonhosted.org/packages/35/8f/892d2e1bcfceb7ee3f9b055ac4bb111e31d25f0a38c7f44d1d59bf7a501a/safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n","  Downloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n","  Obtaining dependency information for huggingface-hub<1.0,>=0.19.3 from https://files.pythonhosted.org/packages/a0/0a/02ac0ae1047d97769003ff4fb8e6717024f3f174a5d13257415aa09e13d9/huggingface_hub-0.20.1-py3-none-any.whl.metadata\n","  Downloading huggingface_hub-0.20.1-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.3)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.14.1)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\n","Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\n","Collecting pyarrow-hotfix (from datasets)\n","  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\n","  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.3)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\n","Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.10.0)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.5)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\n","Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.0.0)\n","Collecting tyro>=0.5.11 (from trl)\n","  Obtaining dependency information for tyro>=0.5.11 from https://files.pythonhosted.org/packages/c5/11/abdf67467d06713b431618732a43f82d1b1f02120107b05a789afbcdf54d/tyro-0.6.0-py3-none-any.whl.metadata\n","  Downloading tyro-0.6.0-py3-none-any.whl.metadata (7.5 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n","INFO: pip is looking at multiple versions of tokenizers to determine which version is compatible with other requirements. This could take a while.\n","Collecting tokenizers<0.19,>=0.14 (from transformers)\n","  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/eb/3d/eee5f3c572a3f4db2ebabf5bd4f284f356078a5b5d27e6229b4450d5c5e4/tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n","  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\n","Requirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (0.15)\n","Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (13.5.2)\n","Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n","  Obtaining dependency information for shtab>=1.5.6 from https://files.pythonhosted.org/packages/40/ad/7227da64498eaa7abecee4311008f70869e156014b3270cec36e2e70cd31/shtab-1.6.5-py3-none-any.whl.metadata\n","  Downloading shtab-1.6.5-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n","Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n","Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.0)\n","Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading datasets-2.15.0-py3-none-any.whl (521 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading peft-0.7.1-py3-none-any.whl (168 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bitsandbytes-0.41.3.post2-py3-none-any.whl (92.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading trl-0.7.4-py3-none-any.whl (133 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.9/133.9 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading huggingface_hub-0.20.1-py3-none-any.whl (330 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading tyro-0.6.0-py3-none-any.whl (100 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n","Downloading shtab-1.6.5-py3-none-any.whl (13 kB)\n","Installing collected packages: bitsandbytes, shtab, safetensors, pyarrow-hotfix, huggingface-hub, tyro, tokenizers, accelerate, transformers, datasets, trl, peft\n","  Attempting uninstall: safetensors\n","    Found existing installation: safetensors 0.4.0\n","    Uninstalling safetensors-0.4.0:\n","      Successfully uninstalled safetensors-0.4.0\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.17.3\n","    Uninstalling huggingface-hub-0.17.3:\n","      Successfully uninstalled huggingface-hub-0.17.3\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.14.1\n","    Uninstalling tokenizers-0.14.1:\n","      Successfully uninstalled tokenizers-0.14.1\n","  Attempting uninstall: accelerate\n","    Found existing installation: accelerate 0.24.1\n","    Uninstalling accelerate-0.24.1:\n","      Successfully uninstalled accelerate-0.24.1\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.35.0\n","    Uninstalling transformers-4.35.0:\n","      Successfully uninstalled transformers-4.35.0\n","  Attempting uninstall: datasets\n","    Found existing installation: datasets 2.1.0\n","    Uninstalling datasets-2.1.0:\n","      Successfully uninstalled datasets-2.1.0\n","Successfully installed accelerate-0.25.0 bitsandbytes-0.41.3.post2 datasets-2.15.0 huggingface-hub-0.20.1 peft-0.7.1 pyarrow-hotfix-0.6 safetensors-0.4.1 shtab-1.6.5 tokenizers-0.15.0 transformers-4.36.2 trl-0.7.4 tyro-0.6.0\n"]}],"source":["!pip install -q transformers datasets peft accelerate bitsandbytes trl safetensors ipywidgets huggingface_hub scipy -U"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T16:31:36.424395Z","iopub.status.busy":"2023-12-20T16:31:36.423643Z","iopub.status.idle":"2023-12-20T16:31:56.453132Z","shell.execute_reply":"2023-12-20T16:31:56.452215Z","shell.execute_reply.started":"2023-12-20T16:31:36.424343Z"},"id":"ynrvfH87vyka","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["<frozen importlib._bootstrap>:241: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n","/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n","  warnings.warn(\n"]}],"source":["from datasets import load_dataset\n","from random import randrange\n","\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n","from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM\n","\n","from trl import SFTTrainer"]},{"cell_type":"markdown","metadata":{"id":"qa80IqnWJL_m"},"source":["## Setting Global Parameters"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T16:31:56.456675Z","iopub.status.busy":"2023-12-20T16:31:56.456356Z","iopub.status.idle":"2023-12-20T16:31:56.466954Z","shell.execute_reply":"2023-12-20T16:31:56.466005Z","shell.execute_reply.started":"2023-12-20T16:31:56.456646Z"},"id":"3qSsWFb6JOLG","trusted":true},"outputs":[],"source":["# The model that you want to train from the Hugging Face hub\n","model_id = \"codellama/CodeLlama-7b-Instruct-hf\"\n","# The instruction dataset to use\n","codecontest_name = \"deepmind/code_contests\"\n","prompt_dataset_name = \"HoangLe1312/codecontest-prompt\"\n","# Dataset split\n","dataset_split = \"train\"\n","# Fine-tuned model name\n","new_model = \"llama-2-7b-int4-codeforces-20k\"\n","# Huggingface repository\n","hf_model_repo = \"HoangLe1312/\" + new_model\n","# Load the entire model on the GPU 0\n","device_map = \"auto\"\n","\n","################################################################################\n","# bitsandbytes parameters\n","################################################################################\n","# Activate 4-bit precision base model loading\n","use_4bit = True\n","# Compute dtype for 4-bit base models\n","bnb_4bit_compute_dtype = \"float16\"\n","# Quantization type (fp4 or nf4)\n","bnb_4bit_quant_type = \"nf4\"\n","# Activate nested quantization for 4-bit base models (double quantization)\n","use_double_nested_quant = False\n","\n","################################################################################\n","# QLoRA parameters\n","################################################################################\n","# LoRA attention dimension\n","lora_r = 16\n","# Alpha parameter for LoRA scaling\n","lora_alpha = 16\n","# Dropout probability for LoRA layers\n","lora_dropout = 0.05\n","\n","################################################################################\n","# TrainingArguments parameters\n","################################################################################\n","# Output directory where the model predictions and checkpoints will be stored\n","output_dir = new_model\n","# Number of training epochs\n","num_train_epochs = 2\n","# Enable fp16/bf16 training (set bf16 to True with an A100)\n","fp16 = True\n","bf16 = False\n","# Batch size per GPU for training\n","batch_size = 128\n","per_device_train_batch_size = 1\n","# Number of update steps to accumulate the gradients for\n","gradient_accumulation_steps = batch_size // per_device_train_batch_size\n","# Enable gradient checkpointing\n","gradient_checkpointing = False\n","# Maximum gradient normal (gradient clipping)\n","max_grad_norm = 0.3\n","# Initial learning rate (AdamW optimizer)\n","learning_rate = 3e-4 #1e-5\n","# Weight decay to apply to all layers except bias/LayerNorm weights\n","weight_decay = 0.001\n","# Optimizer to use\n","optim = \"paged_adamw_32bit\"\n","# Learning rate schedule\n","lr_scheduler_type = \"constant\" # \"constant\"\n","# Number of training steps (overrides num_train_epochs)\n","max_steps = -1\n","# Ratio of steps for a linear warmup (from 0 to learning rate)\n","warmup_ratio = 0.03\n","# Group sequences into batches with same length\n","# Saves memory and speeds up training considerably\n","group_by_length = False\n","# Save checkpoint every X updates steps\n","save_steps = 0\n","# Log every X updates steps\n","logging_steps = 25\n","# Disable tqdm\n","disable_tqdm = True\n","\n","################################################################################\n","# SFTTrainer parameters\n","################################################################################\n","# Maximum sequence length to use\n","max_seq_length = 512 #None\n","# Pack multiple short examples in the same input sequence to increase efficiency\n","packing = True #True #False\n"]},{"cell_type":"markdown","metadata":{"id":"Z3pJIh4h3Usa"},"source":["## Connect to Huggingface Hub"]},{"cell_type":"markdown","metadata":{"id":"UYuRBtyKGgly"},"source":["You can log in to Hugging Face Hub interactively"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZkpT7IiG3Ywk","outputId":"ed0588bf-b7b1-418e-cd4d-3642a40d9acd","trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"OBljdiQhGuOV"},"source":["Or you can provide .env file containing the Hugging Face token"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T16:31:56.468590Z","iopub.status.busy":"2023-12-20T16:31:56.468284Z","iopub.status.idle":"2023-12-20T16:31:56.631835Z","shell.execute_reply":"2023-12-20T16:31:56.630839Z","shell.execute_reply.started":"2023-12-20T16:31:56.468563Z"},"id":"GgZlM-ov461d","outputId":"6d4df447-b9b4-4bc1-f0fc-eb6c09dd418c","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n","Token is valid (permission: read).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["from huggingface_hub import login\n","\n","# Login to the Hugging Face Hub\n","login('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')"]},{"cell_type":"markdown","metadata":{"id":"HJjE6hP3vt_Z"},"source":["## Load the dataset with the instruction set"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T16:31:56.633882Z","iopub.status.busy":"2023-12-20T16:31:56.633189Z","iopub.status.idle":"2023-12-20T16:38:51.625479Z","shell.execute_reply":"2023-12-20T16:38:51.624599Z","shell.execute_reply.started":"2023-12-20T16:31:56.633842Z"},"id":"qbbH23N9vXh2","outputId":"ed782160-af5c-499d-e626-aec05df4ab7a","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d02aa6566abc4500aed2b32f5154cfee","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/13.0k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9dc90d369e4244d8af7a3befceb750c4","version_major":2,"version_minor":0},"text/plain":["Resolving data files:   0%|          | 0/39 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1aef3fe4b07d43a997b2c3559dd87181","version_major":2,"version_minor":0},"text/plain":["Downloading metadata:   0%|          | 0.00/4.52k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5ea0d041b35641d59f4bb367c37fa994","version_major":2,"version_minor":0},"text/plain":["Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"33ad5cc6c1ba4485bdabb15d27e1aee0","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/180M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3d1db758c686441fa1d913c897434811","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/209M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1657dd7c55544a01a04da0b637295622","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/227M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7e188350063e4d36abbb1e4dcdbe3878","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/181M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dba2c2da110748289e758c7b91380f82","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/195M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c6c2a8014334499f9e56a848e87751b4","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/174M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"33b86543eded428b9d13dcd3ea1a0273","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/186M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"37f2a4aa07214f19b01b82e3915fff73","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/172M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4688c4f513854bc59769a2a710fadbce","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/200M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a4340e5cb2354a68937101abe9c3a812","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/205M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dcac0bbdc9ed4b8fb0d1f0261dd54612","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/178M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bbd440004c38473c8afea08739aa1fb3","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/164M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b9ceae80b2a1480c8392a30ae697a3d8","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/200M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fceebec1d66e4f2ea9aa11ccbec2f416","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/197M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4dbe7c2406c04ef4afe6c113c68236c1","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/211M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fb902d71f5e64e42860610f3fe63d7d2","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/179M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"da1384936fd6469595071aad6992a5d1","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/202M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d0ac936de1c34705be42a89928d56b12","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/169M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ada07e98eb3747b599b7c8252a3812a0","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/185M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"21704baf2c504846ac6077ed624632b2","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/191M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ed580b1ef6054ebb8b41fa44b0587a3a","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/211M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5ec2cddab3044c2698467d876ca32a36","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/181M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"af3d3c6dc9824c11a6abf0a902bd6b19","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/194M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5253f4cf14be4992ae381df679ac1392","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/176M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c8a6e4c01e4b47d8bf33718e43068014","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/181M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0b58ed30ecde44d78778aaceb3c2d56f","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/206M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"144307a35b4e45d29d8deb159626f31b","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/189M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4b9ff47f106c4312a41eeffe6887d9f0","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/217M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2f4e820df0e34cfe94b9a2643e270f42","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/179M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cd4b76ad3b4d408894c3bc17d73017f0","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/198M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2c48a9b68ed94403bb14b4f3305d6f3e","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/223M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2ae394b880814e939d5851ec58dd6019","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/181M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"639a27ad8b6a4f07ae5400dc1c01bd49","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/186M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9424750f4b384e9f85df7ee94a6e44b5","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/204M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8780247812fa46acb0fd4280fab4c233","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/188M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"704823dab82946d69b39e689c283fefc","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/151M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"966a9fc1d226453abe73b22b754a8135","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/204M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7c5d60cdc5ca4ed889bee991ee093ed0","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/231M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eff2cf7ec8b74f8ea5793cae96c9d621","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/204M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"26fe79057dd84147bf5cb53a832a0b18","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/63.1M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eb19980faeca44aea9fb44f07aba1cff","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/51.8M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ede2cf4c88094d6fa4e994dfdc4558df","version_major":2,"version_minor":0},"text/plain":["Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"57a7dfed96774bf481ff2023ebb116b1","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/13328 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5103c09075db42caa35da7a4700373f8","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/165 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6911319f38af4175bb21a24c40f7bd1d","version_major":2,"version_minor":0},"text/plain":["Generating valid split:   0%|          | 0/117 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Load dataset from the hub\n","codecontest = load_dataset(codecontest_name, split=dataset_split, download_mode='force_redownload')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uQTTAnGnHG8A","outputId":"65286fab-09ca-41cb-89d0-d06cdacc938f","trusted":true},"outputs":[],"source":["def filter_function(sample):\n","    return sample['source'] == 2 and \\\n","        len(sample['public_tests']['input']) > 0 and \\\n","        len(sample['private_tests']['input']) > 0 and \\\n","        sample['cf_contest_id'] > 0 and \\\n","        sample['cf_index'] != '' and \\\n","        sample['input_file'] == '' and \\\n","        sample['output_file'] == ''"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T16:38:51.627653Z","iopub.status.busy":"2023-12-20T16:38:51.626827Z","iopub.status.idle":"2023-12-20T16:40:36.847782Z","shell.execute_reply":"2023-12-20T16:40:36.846735Z","shell.execute_reply.started":"2023-12-20T16:38:51.627616Z"},"id":"WdlOkzJyFEKq","outputId":"3b6f7f4d-f87e-4824-848e-d80b8c6476c3","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"24a9318d9d29472daa283ab02785361f","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/13328 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["codecontest = codecontest.filter(filter_function)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T16:40:36.852487Z","iopub.status.busy":"2023-12-20T16:40:36.852052Z","iopub.status.idle":"2023-12-20T16:40:36.859802Z","shell.execute_reply":"2023-12-20T16:40:36.858627Z","shell.execute_reply.started":"2023-12-20T16:40:36.852448Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['name', 'description', 'public_tests', 'private_tests', 'generated_tests', 'source', 'difficulty', 'solutions', 'incorrect_solutions', 'cf_contest_id', 'cf_index', 'cf_points', 'cf_rating', 'cf_tags', 'is_description_translated', 'untranslated_description', 'time_limit', 'memory_limit_bytes', 'input_file', 'output_file'],\n","    num_rows: 4135\n","})"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["descriptions = dict()\n","for sample in codecontest:\n","    contest = sample['cf_contest_id']\n","    index = sample['cf_index']\n","    description = sample['description']\n","    if contest not in descriptions:\n","        descriptions[contest] = dict()\n","    descriptions[contest][index] = description    "]},{"cell_type":"markdown","metadata":{"id":"o4DJE2xGwKcl"},"source":["To fine-tune our model, we need to convert our structured examples into a collection of tasks described via instructions. We define a formatting_function that takes a sample and returns a string with our instruction format."]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T17:09:59.611127Z","iopub.status.busy":"2023-12-20T17:09:59.610355Z","iopub.status.idle":"2023-12-20T17:09:59.618175Z","shell.execute_reply":"2023-12-20T17:09:59.617043Z","shell.execute_reply.started":"2023-12-20T17:09:59.611095Z"},"id":"Qw5nDtwmMDUU","trusted":true},"outputs":[],"source":["def format_instruction(sample):\n","    return f\"\"\"### Instruction:\n","You are a contestant in a programming contest. You have to solve the following problem in the contest.\n","\n","### Description:\n","{descriptions[sample['contest']][sample['index']]}\n","\n","{sample['prompt']}\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sPfERkUObRj8","trusted":true},"outputs":[],"source":["del codecontest\n","import gc\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"na-L0aobwYeo","outputId":"1eb3d578-eb0e-4e89-c638-4f2f07d8450c","trusted":true},"outputs":[],"source":["dataset = load_dataset(prompt_dataset_name, split=dataset_split, download_mode='force_redownload')\n","dataset = dataset.select(range(1000))"]},{"cell_type":"markdown","metadata":{"id":"y5beyTxUwtd9"},"source":["## Instruction fine-tune a Llama 2 model using trl and the SFTTrainer"]},{"cell_type":"markdown","metadata":{"id":"RJivw-mLwyDI"},"source":["We will use the recently introduced method in the paper \"QLoRA: Quantization-aware Low-Rank Adapter Tuning for Language Generation\" by Tim Dettmers et al. QLoRA is a new technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance.\n","\n","Quantize the pre-trained model to 4 bits and freeze it.\n","Attach small, trainable adapter layers. (LoRA)\n","Finetune only the adapter layers while using the frozen quantized model for context."]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T16:40:36.892939Z","iopub.status.busy":"2023-12-20T16:40:36.892607Z","iopub.status.idle":"2023-12-20T16:40:36.901989Z","shell.execute_reply":"2023-12-20T16:40:36.901195Z","shell.execute_reply.started":"2023-12-20T16:40:36.892911Z"},"id":"5-OL7AW-xE_r","trusted":true},"outputs":[],"source":["# Get the type\n","compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n","\n","# BitsAndBytesConfig int-4 config\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=use_4bit,\n","    bnb_4bit_use_double_quant=use_double_nested_quant,\n","    bnb_4bit_quant_type=bnb_4bit_quant_type,\n","    bnb_4bit_compute_dtype=compute_dtype\n",")"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T16:40:36.903554Z","iopub.status.busy":"2023-12-20T16:40:36.903151Z","iopub.status.idle":"2023-12-20T16:42:45.290531Z","shell.execute_reply":"2023-12-20T16:42:45.289516Z","shell.execute_reply.started":"2023-12-20T16:40:36.903517Z"},"id":"d_GhlJY7wnn6","outputId":"37304a9a-689c-4f2a-b324-852bc4edd52f","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c94e8d7b090d4fb1b888a41e74783bb5","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/749 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e6a16edb16d64760b0b108f2a4b65b0e","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2ff70dbef7da493a880af6894ff0ed83","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e0c615e938ce4708b5704bcf27ad1ada","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2bb5a306b27f4a229e4254cbde57da4a","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/646 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d07da42e626c47d1b541f69d1b658565","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"03b880305fc049d49524e41ef6b0540c","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7aa9334c78b446a68f4af91ecddd2e59","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dccab4b1f7874da4b599f76af662ca91","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"28e8f68f13df45c0a2b14e95abe24470","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"57b3b4cbdecb46e69f3fa84c02cc73e0","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import LlamaForCausalLM, CodeLlamaTokenizer\n","from peft import (\n","    LoraConfig,\n","    get_peft_model_state_dict,\n",")\n","import sys\n","\n","tokenizer = CodeLlamaTokenizer.from_pretrained(model_id, trust_remote_code=True)\n","tokenizer.add_eos_token = True\n","tokenizer.pad_token_id = 0\n","tokenizer.padding_side = \"left\"\n","\n","model = LlamaForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, use_cache=False, device_map=device_map, load_in_8bit=True, trust_remote_code=True)\n","model.config.pretraining_tp = 1\n","model.config.use_cache = False\n","\n","old_state_dict = model.state_dict\n","model.state_dict = (lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())).__get__(\n","    model, type(model)\n",")\n","if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n","    print(\"compiling the model\")\n","    model = torch.compile(model)"]},{"cell_type":"markdown","metadata":{"id":"4UHudDy8xbe9"},"source":["The SFTTrainer supports a native integration with peft, which makes it super easy to efficiently instruction tune LLMs. We only need to create our LoRAConfig and provide it to the trainer."]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T16:42:45.292617Z","iopub.status.busy":"2023-12-20T16:42:45.292278Z","iopub.status.idle":"2023-12-20T16:42:45.298066Z","shell.execute_reply":"2023-12-20T16:42:45.296731Z","shell.execute_reply.started":"2023-12-20T16:42:45.292587Z"},"id":"8JwVGA88xNrg","trusted":true},"outputs":[],"source":["# LoRA config based on QLoRA paper\n","peft_config = LoraConfig(\n","        lora_alpha=lora_alpha,\n","        lora_dropout=lora_dropout,\n","        r=lora_r,\n","        bias=\"none\",\n","        task_type=\"CAUSAL_LM\",\n","        target_modules=[\n","                \"q_proj\",\n","                \"k_proj\",\n","                \"v_proj\",\n","                \"o_proj\",\n","        ],\n",")\n","\n","# Not necessary when using SFTTrainer\n","# prepare model for training\n","# model = prepare_model_for_kbit_training(model)\n","# model = get_peft_model(model, peft_config)"]},{"cell_type":"markdown","metadata":{"id":"gz3bAOynxt7Z"},"source":["Before we can start our training we need to define the hyperparameters (TrainingArguments) we want to use"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T16:42:45.299939Z","iopub.status.busy":"2023-12-20T16:42:45.299536Z","iopub.status.idle":"2023-12-20T16:42:45.319784Z","shell.execute_reply":"2023-12-20T16:42:45.318733Z","shell.execute_reply.started":"2023-12-20T16:42:45.299901Z"},"id":"hbCaZbitxuW7","trusted":true},"outputs":[],"source":["# Define the training arguments\n","args = TrainingArguments(\n","    output_dir=output_dir,\n","    num_train_epochs=num_train_epochs,\n","    per_device_train_batch_size=per_device_train_batch_size, # 6 if use_flash_attention else 4,\n","    gradient_accumulation_steps=gradient_accumulation_steps,\n","    gradient_checkpointing=gradient_checkpointing,\n","    optim=optim,\n","    #save_steps=save_steps,\n","    logging_steps=logging_steps,\n","    save_strategy=\"epoch\",\n","    learning_rate=learning_rate,\n","    weight_decay=weight_decay,\n","    fp16=fp16,\n","    bf16=bf16,\n","    max_grad_norm=max_grad_norm,\n","    warmup_ratio=warmup_ratio,\n","    #max_steps=max_steps,\n","    group_by_length=group_by_length,\n","    lr_scheduler_type=lr_scheduler_type,\n","    disable_tqdm=disable_tqdm,\n","    report_to=\"tensorboard\",\n","    seed=42\n",")"]},{"cell_type":"markdown","metadata":{"id":"1XTnGAYRy-wZ"},"source":["We now have every building block we need to create our SFTTrainer to start then training our model."]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T17:10:03.559251Z","iopub.status.busy":"2023-12-20T17:10:03.558852Z","iopub.status.idle":"2023-12-20T17:10:04.100675Z","shell.execute_reply":"2023-12-20T17:10:04.099644Z","shell.execute_reply.started":"2023-12-20T17:10:03.559218Z"},"id":"GKRtirQOy_P5","outputId":"0907dea3-2c79-4bb4-845d-2ca32b0faa68","trusted":true},"outputs":[],"source":["# Create the trainer\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    packing=packing,\n","    formatting_func=format_instruction,\n","    args=args,\n",")"]},{"cell_type":"markdown","metadata":{"id":"L3StRhnVzQfp"},"source":["Start training our model by calling the train() method on our Trainer instance."]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2023-12-20T17:10:05.835332Z","iopub.status.busy":"2023-12-20T17:10:05.834920Z","iopub.status.idle":"2023-12-20T17:17:05.318238Z","shell.execute_reply":"2023-12-20T17:17:05.316712Z","shell.execute_reply.started":"2023-12-20T17:10:05.835290Z"},"id":"I-XPLvS7zQ4n","outputId":"097da356-9317-4ef2-dec5-381d55e9d985","trusted":true},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[31], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# there will not be a progress bar since tqdm is disabled\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 280\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1854\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1853\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1854\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1857\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1859\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1860\u001b[0m ):\n\u001b[1;32m   1861\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2744\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2742\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2743\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2746\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:1903\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1902\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1903\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1904\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1905\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import gc\n","gc.collect()\n","# train\n","trainer.train() # there will not be a progress bar since tqdm is disabled"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-20T16:42:48.948855Z","iopub.status.idle":"2023-12-20T16:42:48.949237Z","shell.execute_reply":"2023-12-20T16:42:48.949068Z","shell.execute_reply.started":"2023-12-20T16:42:48.949052Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["# save model in local\n","gc.collect()\n","trainer.save_model()"]},{"cell_type":"markdown","metadata":{"id":"QsiRfb0bz7fh"},"source":["## Merge the model and the adapters and save it"]},{"cell_type":"markdown","metadata":{"id":"TxWD7KsvKWF1"},"source":["When running in a T4 instance we have to clean the memory"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-20T16:42:48.950820Z","iopub.status.idle":"2023-12-20T16:42:48.951202Z","shell.execute_reply":"2023-12-20T16:42:48.951037Z","shell.execute_reply.started":"2023-12-20T16:42:48.951020Z"},"id":"eKr5H6dzL97a","trusted":true},"outputs":[],"source":["# Empty VRAM\n","del model\n","del trainer\n","del dataset\n","del bnb_config\n","del peft_config\n","del compute_dtype\n","\n","import gc\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-20T16:42:48.952974Z","iopub.status.idle":"2023-12-20T16:42:48.953453Z","shell.execute_reply":"2023-12-20T16:42:48.953243Z","shell.execute_reply.started":"2023-12-20T16:42:48.953221Z"},"id":"R1h7kunlRuWE","trusted":true},"outputs":[],"source":["torch.cuda.empty_cache() # PyTorch thing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-20T16:42:48.955206Z","iopub.status.idle":"2023-12-20T16:42:48.955634Z","shell.execute_reply":"2023-12-20T16:42:48.955435Z","shell.execute_reply.started":"2023-12-20T16:42:48.955414Z"},"id":"70Nd6txqMeIt","trusted":true},"outputs":[],"source":["gc.collect()"]},{"cell_type":"markdown","metadata":{"id":"BOEDlZDyKg5A"},"source":["Reload the trained and saved model and merge it then we can save the whole model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-20T16:42:48.957328Z","iopub.status.idle":"2023-12-20T16:42:48.957818Z","shell.execute_reply":"2023-12-20T16:42:48.957586Z","shell.execute_reply.started":"2023-12-20T16:42:48.957562Z"},"id":"Fxamw2PPL804","trusted":true},"outputs":[],"source":["from peft import AutoPeftModelForCausalLM\n","\n","new_model = AutoPeftModelForCausalLM.from_pretrained(\n","    args.output_dir,\n","    low_cpu_mem_usage=True,\n","    return_dict=True,\n","    torch_dtype=torch.float16,\n","    device_map=device_map,\n",")\n","\n","# Merge LoRA and base model\n","gc.collect()\n","merged_model = new_model.merge_and_unload()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-20T16:42:48.958880Z","iopub.status.idle":"2023-12-20T16:42:48.959337Z","shell.execute_reply":"2023-12-20T16:42:48.959106Z","shell.execute_reply.started":"2023-12-20T16:42:48.959085Z"},"trusted":true},"outputs":[],"source":["# Save the merged model\n","gc.collect()\n","merged_model.save_pretrained(\"merged_model\",safe_serialization=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-20T16:42:48.960958Z","iopub.status.idle":"2023-12-20T16:42:48.961373Z","shell.execute_reply":"2023-12-20T16:42:48.961191Z","shell.execute_reply.started":"2023-12-20T16:42:48.961150Z"},"trusted":true},"outputs":[],"source":["gc.collect()\n","tokenizer.save_pretrained(\"merged_model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-12-20T16:42:48.962720Z","iopub.status.idle":"2023-12-20T16:42:48.963073Z","shell.execute_reply":"2023-12-20T16:42:48.962916Z","shell.execute_reply.started":"2023-12-20T16:42:48.962899Z"},"id":"4NmPKCewTb51","trusted":true},"outputs":[],"source":["# push merged model to the hub\n","from huggingface_hub import login\n","# Login to the Hugging Face Hub\n","login('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')\n","\n","gc.collect()\n","merged_model.push_to_hub(hf_model_repo)\n","gc.collect()\n","tokenizer.push_to_hub(hf_model_repo)\n","gc.collect()"]},{"cell_type":"markdown","metadata":{"id":"s-W6-OfOWAHV"},"source":["## Test the merged model"]},{"cell_type":"markdown","metadata":{"id":"9T37lCZozl0B"},"source":["It is time to check our model performance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lgcWF3g3WCxQ","trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pQoevWQfdXsi","trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"NPFsF-OrfBZc"},"source":["## Load the model from the HF Hub and test it"]},{"cell_type":"markdown","metadata":{"id":"yYdbAKY8L29E"},"source":["Finally we download the created model from the hub and test it to make sure it works fine!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gRcpjHQ0sdMy","trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WeDfZ3G-qv1m","trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30580,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
